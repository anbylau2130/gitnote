# 前提
K近邻算法是需要做标准化处理的

# 定义
如果一个样本在特征空间中的k个相似（即特征空间中最临近）的样本中的大多数属于某一个类型 ，则该样本就属于这个类别。
KNN算法最总石油Cover和Hart提出的一种算法# 公式

# 思想
相似的样本，特征之间的值应该都是近似的

# 公式
两个样本的距离可以使用如下公式计算，又叫欧氏距离

![](https://raw.githubusercontent.com/anbylau2130/gitnoteImages/master/gitnoteImages/2019/04/02/5c4e99955eb4f16cf0000000.png)

# K近邻算法API

- sklearn.neighbors.KNeighborsClasssifier(n_neighbors=5,algorithm="auto")
    - n_neightbors:int 可选，默认=5，k_neighbors查询默认使用的邻居数
    - algorithm:{“auto”,"ball_tree","kd_tree","brute"},可选用计算最近邻居的算法：“ball_tree”将会使用BallTree，kd_tree将会使用KDTree."auto"将尝试根据传递给ffit方法的值来决定最核实的算法。
    
    - 当k值取很小：容易受异常点影响
    - k值取很大：容易受k值数量波动

# 优点
简单，易于理解，易于实现,无需估计参数，无需训练

# 缺点

懒惰算法，对测试样本分类时的计算量大，内存开销大
必须指定k值，k值选择不当则分类精确度不能保证

# 使用场景

小数据场景，几千到几万样本，具体场景根据业务去测试


```
 from sklearn.model_selection import  train_test_split
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.preprocessing import StandardScaler
    import pandas as pd

    data=pd.read_csv("d:\\data\\train.csv")

    time_value=pd.to_datetime(data["time"],unit="s")

    #将datetime转换为dict
    time_value=pd.DatetimeIndex(time_value)

    # data中增加dayofweek，hour，month列
    data["dayofweek"]=time_value.dayofweek
    data["hour"]=time_value.hour
    data["month"]=time_value.month

    #data中删除time、列
    data=data.drop(["time"],axis=1)

    place_count=data.groupby("place_id").count()

    places = data[place_count.row_id>3].reset_index()

    data=data[data["place_id"].isin(places.place_id)]

    #将数据分为目标值和特征值
    targets=data["place_id"]
    samples = data.drop('place_id',axis=1)
    # 进行数据分割0.25为测试集，0.75为训练集
    test_trains,test_test,trains_trains,trains_test=train_test_split(samples,targets,test_size=0.25)

    # 标准化处理
    std=StandardScaler()
    std.fit_transform(trains_trains)
    std.fit_transform(test_trains)

    # 算法处理
    knn = KNeighborsClassifier(n_neighbors=5)
    knn.fit(test_trains,trains_trains)
    # 得出 测试集 结果
    knn.predict(test_test)

    # 得出 准确率
    knn.score(test_test,trains_test)

```
</textarea>
			<!-- markdown -->
			<pre class="content-markdown"># 前提
K近邻算法是需要做标准化处理的

# 定义
如果一个样本在特征空间中的k个相似（即特征空间中最临近）的样本中的大多数属于某一个类型 ，则该样本就属于这个类别。
KNN算法最总石油Cover和Hart提出的一种算法# 公式

# 思想
相似的样本，特征之间的值应该都是近似的

# 公式
两个样本的距离可以使用如下公式计算，又叫欧氏距离

![](https://raw.githubusercontent.com/anbylau2130/gitnoteImages/master/gitnoteImages/2019/04/02/5c4e99955eb4f16cf0000000.png)

# K近邻算法API

- sklearn.neighbors.KNeighborsClasssifier(n_neighbors=5,algorithm="auto")
    - n_neightbors:int 可选，默认=5，k_neighbors查询默认使用的邻居数
    - algorithm:{“auto”,"ball_tree","kd_tree","brute"},可选用计算最近邻居的算法：“ball_tree”将会使用BallTree，kd_tree将会使用KDTree."auto"将尝试根据传递给ffit方法的值来决定最核实的算法。
    
    - 当k值取很小：容易受异常点影响
    - k值取很大：容易受k值数量波动

# 优点
简单，易于理解，易于实现,无需估计参数，无需训练

# 缺点

懒惰算法，对测试样本分类时的计算量大，内存开销大
必须指定k值，k值选择不当则分类精确度不能保证

# 使用场景

小数据场景，几千到几万样本，具体场景根据业务去测试


```
 from sklearn.model_selection import  train_test_split
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.preprocessing import StandardScaler
    import pandas as pd

    data=pd.read_csv("d:\\data\\train.csv")

    time_value=pd.to_datetime(data["time"],unit="s")

    #将datetime转换为dict
    time_value=pd.DatetimeIndex(time_value)

    # data中增加dayofweek，hour，month列
    data["dayofweek"]=time_value.dayofweek
    data["hour"]=time_value.hour
    data["month"]=time_value.month

    #data中删除time、列
    data=data.drop(["time"],axis=1)

    place_count=data.groupby("place_id").count()

    places = data[place_count.row_id>3].reset_index()

    data=data[data["place_id"].isin(places.place_id)]

    #将数据分为目标值和特征值
    targets=data["place_id"]
    samples = data.drop('place_id',axis=1)
    # 进行数据分割0.25为测试集，0.75为训练集
    test_trains,test_test,trains_trains,trains_test=train_test_split(samples,targets,test_size=0.25)

    # 标准化处理
    std=StandardScaler()
    std.fit_transform(trains_trains)
    std.fit_transform(test_trains)

    # 算法处理
    knn = KNeighborsClassifier(n_neighbors=5)
    knn.fit(test_trains,trains_trains)
    # 得出 测试集 结果
    knn.predict(test_test)

    # 得出 准确率
    knn.score(test_test,trains_test)

```
